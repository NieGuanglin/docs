[TOC]

# 基本SQL

## 服务

配置文件路径

/etc/mysql/mysql.conf.d/mysqld.cnf

登录

mysql -uroot -p

mysql -hlocalhost -P3306 -uroot -p



## 库操作

展示所有数据库:

**show databases;**

 

创建数据库:

create database 数据库名 charset=utf8;

展示建库语句:

show create database 数据库名;

设置数据库字符集:

alter database 数据库名 charset=utf8;

 

选择使用的数据库:

**use** **数据库名**;

查看当前使用的数据库:

**select database();**

 

删除数据库:

drop database 数据库名;



## 表操作

展示所有数据表:

**show tables;**

 

创建数据表:

create table 表名(

id int(6) unsigned zerofill primary key not null auto_increment,

name varchar(20) not null,

gender enum("man","woman") default "woman",

age int(3) unsigned default 0

);

展示创表语句:

show create table 表名;

show create table 表名 \G --以键值对的形式展示,不带分号结尾

查看表结构:

desc 表名;

 

对表的字段增加,小改,大改,删除

alter table 表名 add 字段名 类型 约束;

alter table 表名 modify 字段名 类型 约束;

alter table 表名 change 原名 新名 类型 约束;

alter table 表名 drop 字段名;

 

删除数据表:

drop table 表名;



## 数据操作

### **增加数据**

1.全列插入

insert into 表名 values(0,xx,xx,xx,default,...); --全列插入时,顺序与字段顺序一致,即使有默认值,也要用default占位.

2.部分列插入

insert into 表名(字段1,字段2...) values(值1,值2..); --部分列插入时,也是对数据行的完整插入,所以只有设置了默认值或允许为null的字段,才允许省略不写.

3.增加多行时,

无论哪种方式,values后用多个括号即可.



### **删除数据**

1.只是删除数据,或者指定的数据.但是主键值会在原基础上变化.

delete from 表名 where 条件;

2.删除全部数据,同时主键值会清空.相当于初始化表.

truncate 表名;

3.推荐使用逻辑删除,而不用delete.

增加 isdelete 字段,表示是否被"删除".



### **修改数据**

update 表名 set 字段1=值1,字段2=值2... where 条件;



### **查找数据**

**select** <u>显示内容</u> **from** <u>数据源</u> **where** <u>条件</u> <u>附加(group by/order by/limit)</u>

#### 顺序

->from 确定数据源

->where 条件筛选

->group by 分组

->select 显示内容

->order by 排序

->limit 分页



1. 查找所有字段

   select * from 表名;

2. 查找指定字段

   select 字段1,字段2 from 表名;

3. select 的附加之 as起别名 (as可省略)

   给字段起别名

   select 字段1 as 字段1新名,字段2 as 字段2新名 from 表名;

   给表起别名

   select 表别名.字段1,表别名.字段2 from 表名 as 表别名;

4. select 的附加之 distinct去重查找

   <u>显示内容</u>

   select **distinct** 字段1 from 表名;

   select distinct 字段1,字段2 from 表名; --把两个字段作为组合条件,去重. 类似于两个字段作为组合条件分组.

5. select的附加之 聚合函数

   <u>显示内容</u>

   **聚合函数**要么在"显示内容"中使用,要么在分组后的having中充当条件(与group by配合).

   - count

     统计行数

     select count(字段名) from 表名; 

     统计指定字段的行数,默认不统计null.

     使用count(ifnull(字段名,设定值)),设定值为null,不统计null.设定值为其他,按1统计

     =>也就是说count统计行数时,始终忽略null行.

   - max(字段名),min(字段名),sum(字段名),avg(字段名) --默认排除null.

      select max(字段名),min(字段名),sum(字段名),avg(字段名) from 表名;

      select sum(ifnull(字段名,指定值)) from 表名; --如果为null,按指定值代替,再计算.

     自定义小数显示的位

      round(avg(字段名),保留的小数位)

      select round(avg(c_age),2) from t_student;

   - 聚合函数可以出现在分组后的having里.

     select gender,avg(age),group_concat(name) from students group by gender having avg(age) > 30;

     --统计每个分组指定字段的信息集合,每个信息之间用逗号分隔

6. select的数据源之 连接

   <u>数据源</u>

   - 内连接

     from 表1 as 表1别名 inner join 表2 as 表2别名 on 表1别名.字段名=表2别名.字段名;

   - 左连接/右连接

     左连接/右连接有一个"强势地位问题",左连接时,左表的字段名强势,左表存在的数据而右表不存在,则右表补充一个null.

     from 表1 as 表1别名 left/right join 表2 as 表2别名 on 表1别名.字段名=表2别名.字段名;

   - 自连接

     表自身与自身连接,表的结构比较特殊,比如:一些数据的字段值,是另一些数据的主键值.他们是有关联的==>自关联.

     from 表 as 表别名1 inner join 表 as 表别名2 on 表别名1.字段名=表别名2.字段名;

     举个有趣的例子:

     需求:找出河南省辖区内的所有市信息.

     select * from 地域表 as a inner join 地域表 as b on a.pid=b.id where b.name="河南省";

     语句中,一定是用id的那个表的名字叫"河南省",才能查出城市.与on后等号前后的顺序,inner join前后的顺序无关.

     其实,虽然是一个表起了两个名字,但是在自连接中,它们就是有区别的,有标识的.

     ====>**where**, **having**, **on**

7. select 的附加之 where条件查询

   <u>条件</u>

   where 比较(=, !=),逻辑(and, or, not),模糊(like,%,_),范围( between...and...,in (...,...,) ),空判断(is null,is not null)

    %任意多个任意字符

   _一个任意字符

   select * from students where name like "黄%";

   select * from students where name like "黄_";

8. select的子查询

   <u>条件/数据源</u>

   子查询要么充当条件,要么充当数据源.子查询充当数据源时,需要给数据源起一个别名.

   子查询可以独立存在,是一条完整的select语句.它的语法必须完整,可以用来初步检查笔误.

   需求:挑选出年龄最大的女性,在她们中再挑选班级ID最小的人员信息.

   select * from (select * from t_student where c_age=(select max(c_age) from t_student where c_gender="女")) as a 

   where a.c_class_id=

   (select min(b.c_class_id) from (select * from t_student where c_age=(select max(c_age) from t_student where c_gender="女")) as b);

   ```sql
   select 
   	* 
   from 
   	(
   		select 
   			* 
   		from 
   			t_student 
   		where 
   			c_age=
   				(
   					select 
   						max(c_age) 
   					from 
   						t_student 
   					where 
   						c_gender="女"
   				)
   	) as a 
   where 
   	a.c_class_id=
   		(
   			select 
   				min(b.c_class_id) 
   			from 
   				(
   					select 
   						* 
   					from 
   						t_student 
   					where c_age=
   						(
   							select 
   								max(c_age) 
   							from 
   								t_student 
   							where 
   								c_gender="女"
   						)
   				) as b
   		);
   ```

   

9. select 的附加之 where后面加order by 字段 asc/desc

   <u>附加</u>

   select * from 表名 where 条件 order by 字段1 asc/desc [字段2 asc/desc ...];

   其中asc升序为默认,可以省略.

   asc:ascend

   desc:descend

10. select的附加之 where后面加limit 分页查询

    <u>附加</u>

    select * from 表名 where 条件 limit 起始行索引(不等于id),显示行数;

11. select的附加之 分组

    <u>附加</u>

    group by 字段名[,字段名2...] [having 条件表达式] [with rollup]

    字段名: 是指按照指定字段的值进行分组。

    having 条件表达式: 用来过滤分组后的数据。

    with rollup：在所有记录的最后加上一条记录，显示select查询时聚合函数的统计和计算结果.

    having 与 with rollup 好像只能二选一.???

    - select后字段名与by后字段名要完全一致! 或 使用group_concat(字段名2)

      select 字段名 from 表名 group by 字段名;

      select group_concat(字段名2) from 表名 group by 字段名1;

    - 如需增加显示的信息,需要使用group_concat(字段名).显示的是,在分组内,指定字段的信息集.

      select 字段名,group_concat(字段名2) from 表名 group by 字段名;

    - having只能在group by后用.

      select 字段名,group_concat(字段名2) from 表名 group by 字段名 having 条件;



### 外键约束

#### 新增

它强调的是一种约束关系,所以在新增外键约束前,首先要有该键,要有约束表.

- 已经有表.(先确认已有外键字段)

  alter table *被约束表* add foreign key(*外键字段*) references *约束表*(*主键*);

  alter table *students* add foreign key(*cls_id*) references *classes*(*id*);

-  新建表时即添加外键约束.建议写在所有字段后面.

  foreign key(*外键字段*) references *约束表*(*主键*);

- \*自定义外键约束名,不自定义外键约束名,系统默认 "表名_ibfk_1"

  Alter table 表名 add [constraint 外键约束名] foreign key (外键字段) references 父表(主键);

#### 删除

外键约束名不是外键字段名.外键约束名可以通过 show create table 表名; 查看.

只有取消外键约束,约束表才能被删除.

alter table 表名 drop foreign key 外键约束名;



### 索引

显示已有索引

show index from 表名;

#### 新增

alter table 表名 add index \[idx_字段名\](字段1,字段2...);

[]自定义字段名为可选项,如果不写,默认为字段名.

(多个字段)即为联合索引,遵循"最左原则"

#### 删除

alter table 表名 drop index 索引名;



# 高阶原理

## 基础架构

### server层

Server 层包括**连接器**、查询缓存、**分析器**、**优化器**、**执行器**等。涵盖 MySQL 的大多数核心服务功能，以及所有的内置函数（如日期、时间、数学和加密函数等），所有跨存储引擎的功能都在这一层实现，比如存储过程、触发器、视图等。

- 连接器

  连接命令中的 mysql 是客户端工具，用来跟服务端建立连接。在完成经典的 TCP 握手后，连接器就要开始认证你的身份，这个时候用的就是你输入的用户名和密码。如果用户名密码认证通过，连接器会到权限表里面查出你拥有的权限。之后，这个连接里面的权限判断逻辑，都将依赖于此时读到的权限。

  这就意味着，一个用户成功建立连接后，即使你用管理员账号对这个用户的权限做了修改，也不会影响已经存在连接的权限。修改完成后，只有再新建的连接才会使用新的权限设置。

  全部使用长连接后，你可能会发现，有些时候 MySQL 占用内存涨得特别快，这是因为 MySQL 在执行过程中临时使用的内存是管理在连接对象里面的。这些资源会在连接断开的时候才释放。所以如果长连接累积下来，可能导致内存占用太大，被系统强行杀掉（OOM），从现象看就是 MySQL 异常重启了。

  (可以在每次执行一个比较大的操作后，通过执行 mysql_reset_connection 来重新初始化连接资源。这个过程不需要重连和重新做权限验证，但是会将连接恢复到刚刚创建完时的状态。)

- (查询缓存)

  查询缓存的失效非常频繁，只要有对一个表的更新，这个表上所有的查询缓存都会被清空。MySQL 8.0 版本直接将查询缓存的整块功能删掉了，也就是说 8.0 开始彻底没有这个功能了。

- 分析器

  对 SQL 语句做解析，先会做“词法分析”，再做“语法分析”，还会对语句中的表、字段是否存在进行判断。

- 优化器

  优化器是在表里面有多个索引的时候，决定使用哪个索引；或者在一个语句有多表关联（join）的时候，决定各个表的连接顺序。

- 执行器

  开始执行的时候，要先判断一下你对这个表 T 有没有执行查询的权限。

  如果没有，就会返回没有权限的错误。

  如果有权限，就打开表继续执行。打开表的时候，执行器就会根据表的引擎定义，去使用这个引擎提供的接口。

### 存储引擎层

存储引擎层负责数据的存储和提取。其架构模式是插件式的，支持 InnoDB、MyISAM、Memory 等多个存储引擎。现在最常用的存储引擎是 InnoDB，它从 MySQL 5.5.5 版本开始成为了默认存储引擎。



## 日志系统

### redo log

WAL 的全称是 Write-Ahead Logging，就是先写日志，再写磁盘。

当有一条记录需要更新的时候，InnoDB 引擎就会先**把记录写到 redo log里面，并更新内存**，这个时候更新就算完成了。同时，InnoDB 引擎会在适当的时候，将这个操作记录更新到磁盘里面，而这个更新往往是在系统比较空闲的时候做。

InnoDB 的 redo log 是固定大小的，比如可以配置为一组 4 个文件，每个文件的大小是 1GB，那么这块“粉板”总共就可以记录 4GB 的操作。从头开始写，写到末尾就又回到开头循环写。

有了 redo log，InnoDB 就可以保证即使数据库发生异常重启，之前提交的记录都不会丢失，这个能力称为 crash-safe。

<u>redo log需要存储在磁盘上。写它比直接写数据块效率高？</u>redo log是顺序写，直接写数据块是随机写。

### bin log

- **redo log 是 InnoDB 引擎特有的日志，而 Server 层也有自己的日志，称为 binlog（归档日志）。**

- **redo log 是物理日志，记录的是“在某个数据页上做了什么修改”；binlog 是逻辑日志，记录的是这个语句的原始逻辑，比如“给 ID=2 这一行的 c 字段加 1 ”。**


- **redo log 是循环写的，空间固定会用完；binlog 是可以追加写入的。“追加写”是指 binlog 文件写到一定大小后会切换到下一个，并不会覆盖以前的日志。**

当需要扩容的时候，也就是需要再多搭建一些备库来增加系统的读能力的时候，现在常见的做法也是用全量备份加上应用 binlog 来实现的，

### bin log与redo log的配合

1. 执行器先找引擎取 ID=2 这一行。ID 是主键，引擎直接用树搜索找到这一行。如果 ID=2 这一行所在的数据页本来就在内存中，就直接返回给执行器；否则，需要先从磁盘读入内存，然后再返回。
2. 执行器拿到引擎给的行数据，把这个值加上 1，比如原来是 N，现在就是 N+1，得到新的一行数据，再调用引擎接口写入这行新数据。
3. **引擎将这行新数据更新到内存中**，同时将这个更新操作**记录到 redo log 里面，此时 redo log 处于 prepare 状态**。然后告知执行器执行完成了，随时可以提交事务。
4. **执行器生成这个操作的 binlog**，并把 binlog 写入磁盘。
5. **执行器调用引擎的提交事务接口，引擎把刚刚写入的 redo log 改成提交（commit）状态**，更新完成。

如果不使用“两阶段提交”，那么数据库的状态就有可能和用它的日志恢复出来的库的状态不一致。



## 事务

### ACID

- 原子性，Atomicity：**一个事务（transaction）中的所有操作，或者全部完成，或者全部不完成，不会结束在中间某个环节**。事务在执行过程中发生错误，会被回滚（Rollback）到事务开始前的状态，就像这个事务从来没有执行过一样。即，事务不可分割、不可约简。
- 一致性，Consistency：**在事务开始之前和事务结束以后，数据库的完整性没有被破坏。**这表示写入的资料必须完全符合所有的预设约束、触发器、级联回滚等。
- 隔离性，Isolation：数据库**允许多个并发事务同时对其数据进行读写，隔离性可以防止由于多个事务并发执行而导致数据的不一致**。
- 持久性，Durability：**事务处理结束后，对数据的修改就是永久的**，即便系统故障也不会丢失。

### 隔离级别

- 读未提交，read uncommitted：一个事务还没提交时，它做的变更就能被别的事务看到。
- 读已提交，read committed：一个事务提交之后，它做的变更才会被其他事务看到。
- 可重复读，repeatable read：一个事务执行过程中看到的数据，总是跟这个事务在启动时看到的数据是一致的。
- 串行化，serializable：对于同一行记录，“写”会加“写锁”，“读”会加“读锁”。当出现读写锁冲突的时候，后访问的事务必须等前一个事务执行完成，才能继续执行。

在实现上，数据库里面会创建一个视图，访问的时候以视图的逻辑结果为准。

- RU：直接返回记录上的最新值，没有视图概念。
- **RC**：这个视图是在每个 SQL 语句开始执行的时候创建的。

- **RR**：这个视图是在事务启动时创建的，整个事务存在期间都用这个视图。
- serial：直接用加锁的方式来避免并行访问。



## 索引

为了让一个查询尽量少地读磁盘，就必须让查询过程访问尽量少的数据块。那么，我们就不应该使用二叉树，而是要使用“N 叉”树。这里，“N 叉”树中的“N”取决于数据块的大小。N 叉树由于在读写上的性能优点，以及适配磁盘的访问模式，已经被广泛应用在数据库引擎中了。

> 以 InnoDB 的一个整数字段索引为例，这个 N 差不多是 1200。这棵树高是 4 的时候，就可以存 1200 的 3 次方个值，这已经 17 亿了。考虑到树根的数据块总是在内存中的，一个 10 亿行的表上一个整数字段的索引，查找一个值最多只需要访问 3 次磁盘。其实，树的第二层也有很大概率在内存中，那么访问磁盘的平均次数就更少了。
>
> MySql默认一个节点的长度为16KB，一个整数（bigint）字段索引的长度为 8B，另外每个索引还跟着6B的指向其子树的指针；所以16*1024B/14B ≈ 1170。

在 MySQL 中，索引是在存储引擎层实现的，所以并没有统一的索引标准，即不同存储引擎的索引的工作方式并不一样。

### 覆盖索引

如果非主键索引的叶子节点本身就包含了要查询的字段，那就可以说该非主键索引覆盖了查询需求，因此被称为覆盖索引。

【由于覆盖索引可以**减少回表的次数**】，显著提升查询性能，所以使用覆盖索引是一个常用的性能优化手段。

回表

**在 InnoDB 中，表都是根据主键顺序以索引的形式存放的，这种存储方式的表称为索引组织表。**InnoDB 使用了 B+ 树索引模型，所以数据都是存储在 B+ 树中的。**主键索引的叶子节点存的是整行数据。非主键索引的叶子节点内容是主键的值。**

举例：

- 如果语句是 select * from T where ID=500，即主键查询方式，则只需要搜索 ID 这棵 B+ 树。
- 如果语句是 select * from T where k=5，即普通索引查询方式，则需要先搜索 k 索引树，得到 ID 的值为 500，再到 ID 索引树搜索一次。这个过程称为回表。

基于非主键索引的查询需要多扫描一棵索引树。因此，我们在应用中应该尽量使用主键查询。

### 联合索引

联合索引中，索引项是按照索引定义里面出现的字段顺序排序的，所以它可以复用索引。因为可以支持最左前缀，所以当已经有了 (a,b) 这个联合索引后，一般就不需要单独在 a 上建立索引了。

【联合索引可以兼顾多种查询需求，所以可以**减少索引树的数量**】。

最左前缀：不只是索引的全部定义，只要满足最左前缀，就可以利用索引来加速检索。这个最左前缀可以是联合索引的最左 N 个字段，也可以是字符串索引的最左 M 个字符。

举例：

(a, b, c)建立联合索引，可以支持”where 条件a 条件b 条件c“，还支持”where 条件a 条件b“，还支持”where 条件a“。

### 唯一索引与普通索引

change buffer对写入的影响

优势：当需要更新一个数据页时，如果数据页在内存中就直接更新，而如果这个数据页还没有在内存中的话，在不影响数据一致性的前提下，InnoDB 会将这些更新操作缓存在 change buffer 中，这样就不需要从磁盘中读入这个数据页了。在下次查询需要访问这个数据页的时候，将数据页读入内存，然后执行 change buffer 中与这个页有关的操作。通过这种方式就能保证这个数据逻辑的正确性。

将 change buffer 中的操作应用到原数据页，得到最新结果的过程称为 merge。除了访问这个数据页会触发 merge 外，系统有后台线程会定期 merge。在数据库正常关闭（shutdown）的过程中，也会执行 merge 操作。

显然，如果能够将更新操作先记录在 change buffer，**减少读磁盘**，语句的执行速度会得到明显的提升。而且，数据读入内存是需要占用 buffer pool 的，所以这种方式还能够避免占用内存，提高内存利用率。

**弊端：**普通索引的所有场景，使用 change buffer 都可以起到加速作用吗？

因为 merge 的时候是真正进行数据更新的时刻，而 change buffer 的主要目的就是将记录的变更动作缓存下来，所以在一个数据页做 merge 之前，change buffer 记录的变更越多（也就是这个页面上要更新的次数越多），收益就越大。因此，**对于写多读少的业务**来说，页面在写完以后马上被访问到的概率比较小，此时 change buffer 的使用效果最好。这种业务模型常见的就是账单类、日志类的系统。反过来，假设一个业务的更新模式是写入之后马上会做查询，那么即使满足了条件，将更新先记录在 change buffer，但之后由于马上要访问这个数据页，会立即触发 merge 过程。这样随机访问 IO 的次数不会减少，反而增加了 change buffer 的维护代价。所以，对于这种业务模式来说，change buffer 反而起到了副作用。



普通索引：允许被索引的数据列包含重复的值。

唯一索引：不允许被索引的数据列包含重复的值。

查询：

这两种索引对于查询操作的影响，微乎其微。

更新：

【普通索引】的更新，会使用change buffer机制，提升性能。如果所有的更新后面，都马上伴随着对这个记录的查询，那么你应该关闭 change buffer。而在其他情况下，change buffer 都能提升更新性能。

但是如果业务需要数据侧保证数据的唯一性，那么必须使用唯一索引了。

### 选错索引？

1.选错索引是因为判断扫描行数的时候出了问题。
在数据库里面，扫描行数是影响执行代价的因素之一。扫描的行数越少，意味着访问磁盘数据的次数越少，消耗的 CPU 资源越少。
优化器还会结合是否使用临时表、是否排序等因素进行综合判断。

2.判断扫描行数是基于统计信息。
这个统计信息就是索引的“区分度”。显然，一个索引上不同的值越多，这个索引的区分度就越好。而一个索引上不同的值的个数，我们称之为“基数”（cardinality）。也就是说，这个基数越大，索引的区分度越好。

3.基数是通过抽样，求平均得到的。【show index】查看索引基数。

4.其实索引统计只是一个输入，对于一个具体的语句来说，优化器还要判断，执行这个语句本身要扫描多少行。也许，优化器认为反复回表还不如直接全表扫描。【explain】查看优化器预计的扫描行数。

5.错误的预计扫描行数，与多事务的视图有关。

总结：通过explain查看优化器预计的扫描行数明显比实际情况大，猜测是统计信息出错导致的选错索引。

因为判断扫描行数是基于统计信息，这个统计信息就是索引的“区分度”。显然，一个索引上不同的值越多，这个索引的区分度就越好。而一个索引上不同的值的个数，我们称之为“基数”（cardinality）。也就是说，这个基数越大，索引的区分度越好。基数是通过抽样，求平均得到的。

由于统计信息出错，优化器认为：在一个索引上反复回表还不如直接全表扫描。所以用错了索引。

- 既然是统计信息不对，那就修正。analyze table t 命令，可以用来重新统计索引信息。
- 采用 force index 强行选择一个索引。



## 锁

### 全局锁

实际使用中，基本上都是InnoDB引擎，readonly有太多缺陷，所以最常用的就是mysqldump。全局锁并不常用，这里只是扩展一下知识。

- 加全局读锁

  MySQL 提供了一个加全局读锁的方法，命令是 Flush tables with read lock (FTWRL)。当需要让整个库处于只读状态的时候，可以使用这个命令，之后其他线程的以下语句会被阻塞：数据更新语句（数据的增删改）、数据定义语句（包括建表、修改表结构等）和更新类事务的提交语句。

  全局锁的典型使用场景是，做全库逻辑备份。

- mysqldump

  官方自带的逻辑备份工具是 mysqldump。当 mysqldump 使用参数–single-transaction 的时候，导数据之前就会启动一个事务，来确保拿到一致性视图。而由于 MVCC 的支持，这个过程中数据是可以正常更新的。

  对于 MyISAM 这种不支持事务的引擎，如果备份过程中有更新，总是只能取到最新的数据，那么就破坏了备份的一致性。这时，我们就需要使用 FTWRL 命令了。

- set global readonly=true

  readonly 方式可以让全库进入只读状态，但不建议这么做。

  在有些系统中，readonly 的值会被用来做其他逻辑，比如用来判断一个库是主库还是备库。因此，修改 global 变量的方式影响面更大，不建议使用。

  在异常处理机制上有差异。如果执行 FTWRL 命令（Flush tables with read lock）之后由于客户端发生异常断开，那么 MySQL 会自动释放这个全局锁，整个库回到可以正常更新的状态。而将整个库设置为 readonly 之后，如果客户端发生异常，则数据库就会一直保持 readonly 状态，这样会导致整个库长时间处于不可写状态，风险较高。

### 表级锁

#### 表锁

表锁除了会限制别的线程的读写外，也限定了本线程接下来的操作对象。比如：如果在某个线程 A 中执行 lock tables t1 read, t2 write; 这个语句，则其他线程写 t1、读写 t2 的语句都会被阻塞。同时，线程 A 在执行 unlock tables 之前，也只能执行读 t1、读写 t2 的操作。

在还没有出现更细粒度的锁的时候，表锁是最常用的处理并发的方式。而对于 InnoDB 这种支持行锁的引擎，一般不使用 lock tables 命令来控制并发，毕竟锁住整个表的影响面还是太大。

#### 元数据锁

元数据锁不需要显式使用，在访问一个表的时候会被自动加上。它的作用是，保证读写的正确性。当对一个表做增删改查操作的时候，加 MDL 读锁；当要对表做结构变更操作的时候，加 MDL 写锁。

事务中的 MDL 锁，在语句执行开始时申请，但是**语句结束后并不会马上释放，而会等到整个事务提交后再释放。**

### 行锁

MySQL 的行锁是在引擎层由各个引擎自己实现的。但并不是所有的引擎都支持行锁，比如 MyISAM 引擎就不支持行锁。不支持行锁意味着并发控制只能使用表锁，对于这种引擎的表，同一张表上任何时刻只能有一个更新在执行，这就会影响到业务并发度。InnoDB 是支持行锁的，这也是 MyISAM 被 InnoDB 替代的重要原因之一。

在 InnoDB 事务中，行锁是在需要的时候才加上的，但**并不是不需要了就立刻释放，而是要等到事务结束时才释放。**

#### 避免死锁，死锁检测

1. **如果事务中需要锁多个行，要把最可能造成锁冲突、最可能影响并发度的锁尽量往后放。这样，最可能冲突的锁的时间就最少。**
2. 为了避免过多的死锁检测而导致耗费大量CPU资源，可以在数据库服务端做并发控制。在中间件中，使用队列控制并发度。但这需要有相应的团队。
3. **将最容易冲突的行，拆分成多行。更新时，随机挑选一行；删除时，综合考虑多行。**



