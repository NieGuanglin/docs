[TOC]

# Service

service解决了两个问题：pod的ip不固定；一组pod实例之间的负载均衡。

控制器是维持pod期望的副本数，属于控制层面；service是将pod的服务暴露出去，属于服务发现层面。

有两种访问service的方式：

1. 以service的VIP（也即Virtual IP，虚拟IP）方式。

   当service的IP不是none时，它的IP就是一个VIP。VIP会把请求转发到该svc代理的一个pod上。

   ```bash
   $ kubectl get svc -n svc-test
   common-etcd			ClusterIP	None						<none>	2380/TCP,2379/TCP
   config-manager	ClusterIP	10.103.164.93		<none>	80/TCP
   ```

2. 以service的DNS方式。

   访问\<serviceName>.\<namespaceName>.svc.cluster.local这条DNS记录，就可以访问到该svc所代理的某一个pod。

   在DNS方式下，还分两种处理方法：

   - Normal Service，域名解析到的是service的VIP。

     ```bash
     #获取coredns的pod的IP
     $ kubectl get pod -n kube-system -o wide |grep coredns
     kube-system	coredns-xxxxxxxxx-xxxxx	1/1 Running	0	xxd	172.30.2.204	host1-IP	<none>	<none>
     kube-system	coredns-xxxxxxxxx-xxxxx	1/1 Running	0	xxd	172.30.0.23	host2-IP	<none>	<none>
     kube-system	coredns-xxxxxxxxx-xxxxx	1/1 Running	0	xxd	172.30.1.204	host3-IP	<none>	<none>
     #解析Normal Service的DNS
     $ dig -t A config-manager.svc-test.svc.cluster.local @172.30.2.204
     ...
     ;; ANSWER SECTION:
     config-manager.svc-test.svc.cluster.local.	30 IN	A	10.103.164.93
     ...
     # 查看DNS记录对应的IP，到底是pod的IP，还是svc的IP
     $ kubectl get pod -n svc-test -o wide |grep config-manager
     config-manager-xxxxxxxxxx-xxxxx	2/2	Running	2	xxd	172.30.2.183	host1-IP	<none>	<none>
     config-manager-xxxxxxxxxx-xxxxx	2/2	Running	2	xxd	172.30.2.47	host2-IP	<none>	<none>
     $ kubectl get svc -n svc-test -o wide |grep config-manager
     config-manager	ClusterIP	10.103.164.93 <none>	80/TCP	xxd app=config-manager
     ```

   - Headless Service，域名解析到的直接就是pod的IP。

     ```bash
     #解析Headless Service的DNS
     $ dig -t A common-etcd.svc-test.svc.cluster.local @172.30.2.204
     ...
     ;; ANSWER SECTION:
     common-etcd.svc-test.svc.cluster.local.	30 IN	A	172.30.2.213
     common-etcd.svc-test.svc.cluster.local.	30 IN	A	172.30.0.73
     common-etcd.svc-test.svc.cluster.local.	30 IN	A	172.30.1.216
     ...
     $ kubectl get pod -n svc-test -o wide |grep common-etcd
     common-etcd-0	1/1	Running	0	xxd	172.30.2.213	host1-IP	<none>	<none>
     common-etcd-1	1/1	Running	0	xxd	172.30.0.73	host2-IP	<none>	<none>
     common-etcd-2	1/1	Running	0	xxd	172.30.1.216	host3-IP	<none>	<none>
     ```

   这两种处理方法的区别就是：Headless Service不需要分配一个VIP，直接以DNS记录的方式解析出被代理的pod的IP。

## 标签选择

app或svc通过select的标签，选择pod。

app的select标签是pod的meta标签的子集。

svc的select标签是pod的meta标签的子集。

理论上，要确定与app相关的svc，首先要根据app找到pod，再根据pod的meta标签筛选svc。但是这样太麻烦，而且多数情况下，svc的select标签和app的select标签保持一致。

在用命令行搜索时，要注意：通过客户端搜索时，-l k=v 是对meta标签的过滤，不是select标签。

被select选中的Pod，就称为Service的Endpoints，查看可以用kubectl get ep  \<svcName\>。

注意：只有处于Running状态，且readinessProbe检查通过的Pod，才会出现在Service的Endpoints列表里。当pod出现问题，k8s会自动剔除。

## 类型

### ClusterIP

自动分配一个仅Cluster内部可以访问的VIP，在宿主机上curl是可以访问的。

#### Headless Service

- **\<svcName>.\<namespaceName>.svc.cluster.local**

  该域名将会解析出被代理的pod的ip。

  验证：dig -t A common-etcd.svc-test.svc.cluster.local @coredns的pod的ip

  在answer section中可以看到该svc代理的若干个pod的ip。

- **\<podName>.\<svcName>.\<namespaceName>.svc.cluster.local**

  该域名将会解析出具体的被代理的pod的ip。

  验证：dig -t A common-etcd-0.common-etcd.svc-test.svc.cluster.local @coredns这个pod的ip
  
  在answer section中可以看到该域名对应的pod的ip。

### NodePort

在ClusterIP基础上为svc在每台机器上绑定一个端口，这样就可以通过NodeIP:NodePort来访问该服务。

### LoadBalancer

在NodePort的基础上，借助cloud provider创建一个外部负载均衡器，并将请求转发到NodeIP:NodePort。

### ExternalName

把集群外部的服务引入到集群内部来，在集群内部直接使用。没有任何类型代理被创建。

# Ingress

ingress对象，就是service的service，它是k8s项目对“反向代理”的一种抽象。ingress工作在七层，service工作在四层。一个ingress对象的主要内容，就是一个“反向代理”服务（如Nginx）的配置文件的描述。

**ingress竟然可以跨namespace将自己的rules注入ingress-controller的pod.**

## 工作原理

<img src="https://github.com/NieGuanglin/docs/blob/main/pics/CNCF/k8s/ingress工作原理.png">

<img src="/Users/nieguanglin/pics/CNCF/k8s/ingress工作原理.png" alt="ingress工作原理" style="zoom:100%;" />

1. 首先要有一个svc管理若干pod，这个svc就是常规的ClusterIP模式。

   ```yaml
   apiVersion: v1
   kind: Service
   metadata:
     labels:
       app: a
     name: a
     namespace: svc-test1
   spec:
     ...
     ports:
     - port: 8080
       protocol: TCP
       targetPort: 8080
     selector:
       app: a
     type: ClusterIP
   ...  
   ---
   apiVersion: v1
   kind: Service
   metadata:
     labels:
       app: b
     name: b
     namespace: svc-test2
   spec:
     ...
     ports:
     - name: http
       port: 80
       protocol: TCP
       targetPort: 8085
     selector:
       app: b
     type: ClusterIP
   ...  
   ```

2. ingress的svc是通过nodeport暴露服务，ingress也有自己的deployment和pod。

   ```yaml
   apiVersion: apps/v1
   kind: Deployment
   metadata:
     labels:
       apps: ingress-kong
     name: ingress-kong
     namespace: kong
   spec:
     progessDeadlineSeconds: 600
     replicas: 1
     revisionHistoryLimit: 10
     selector:
       matchLabels:
         app: ingress-kong
     strategy:
       rollingUpdate:
         maxSurge: 25%
         maxUnavailable: 25%
       type: RollingUpdate
     template:
       metadata:
         labels:
           app: ingress-kong
       spec:
         affinity:
           podAntiAffinity:
             requireDuringSchedulingIgnoredDuringExecution:
             - labelSelector:
                 matchExpressions:
                 - key: app
                   operator: In
                   values:
                   - ingress-kong
               topologyKey: kubernetes.io/hostname
         containers:
         - env:
           ...
           image: ...
           imagePullPolicy: IfNotPresent
           lifecycle:
             preStop:
               exec:
                 command:
                 - /bin/sh
                 - -c
                 - kong quit
           livenessProbe: ...
           name: proxy
           ports:
           - containerPort: 8001
             name: admin
             protocol: TCP
           - containerPort: 8000
             name: proxy
             protocol: TCP
           - containerPort: 8443
             name: proxy-ssl
             protocol: TCP
           - containerPort: 9542
             name: metrics
             protocol: TCP
           readinessProbe: ...
           resources: ...
           ...
         - args:
           ...
           env:
           ...
           image: ...
           imagePullPolicy: IfNotPresent
           livenessProbe: ...
           name: ingress-controller
           ports:
           - containerPort: 8080
             name: webhook
             protocol: TCP
           readinessProbe: ...
           ...
         ...  
         volumes:
         - configMap:
             defaultMode: 420
             name: kong-server-blocks
           name: kong-server-blocks
   status:
     ...
   --
   apiVersion: v1
   kind: Service
   metadata:
     name: kong-proxy
     namespace: kong
   spec:
     ...
     ports:
     - name: proxy
       nodePort: 80
       port: 80
       protocol: TCP
       targetPort: 8000
     - name: proxy-ssl
       nodeport: 443
       port: 443
       protocol: TCP
       targetPort: 8443
    selector:
      app: ingress-kong
    type: NodePort
   ```

3. 然后各个服务的Ingress的资源清单中提供host字段，值就是各个服务的域名。ingress对象会将域名规则注入到ingress的pod中。

   ```yaml
   apiVersion: extensions/v1beta1
   kind: Ingress
   metadata:
     labels:
       app: a
     name: a
     namespace: svc-test1
   spec:
     rules:
     - host: a.svc-test1.com
       http:
         paths:
         - backend:
             serviceName: a
             servicePort: 8080
           path: /
     - host: websocket-a.svc-test1.com
       http:
         paths:
         - backend:
             # 该svc是一个nodePort，暴露8080，后端与a svc是同一个后台
             serviceName: websocket-a
             servicePort: 8080
           path: /
   ...
   ---
   apiVersion: extensions/v1beta1
   kind: Ingress
   metadata:
     labels:
       app: b
     name: b
     namespace: svc-test2
   spec:
     rules:
     - host: b.svc-test2.com
       http:
         paths:
         - backend:
             serviceName: b
             servicePort: 80
           path: /
   ...        
   ```

4. 在集群外部，ingres.spec.rules.hosts域名，通过本机host文件将域名解析为hostip；再加上默认的80端口，形成hostip:80。这样就可以访问到ingress的NodePort的svc，进而访问到ingress-controller的pod。再根据域名，访问到了app的ClusterIP的svc，进而访问到真正的后台pod。

# ipvs代理模式

旧版本中，k8s的service是通过iptables实现的。kube-proxy通过iptables处理service的过程，就是在宿主机上设置相当多的iptables规则。当宿主机有大量pod时，就会有大量iptables规则不断地被刷新，会大量占用该宿主机的cpu资源。

ipvs的service实现，很好的解决了这一问题。相比于iptables，ipvs在内核中的实现也是基于NAT，所以在转发这一层上，ipvs并没有显著的性能提升。但是ipvs并不需要在宿主机上为每个pod设置iptables规则，而是把这些规则的处理放在了内核态，极大地降低了维护这些规则的代价。

ipvs模块只负责负载均衡和代理功能。而一个完整的service流程正常工作所需要的包过滤，SNAT等操作，还是要靠iptables实现。只不过这些辅助性的iptables规则数量有限，也不会随着pod数量的增加而增加。

## 工作原理

kube-proxy会在宿主机上创建一个虚拟网卡（kube-ipvs0），并把所有Normal Service的VIP作为它的IP地址。

```bash
$ ip addr
...
kube-ipvs0: <BROADCAST,NOARP> mtu 1500 qdisc noop state DOWN
	link/ether be:dc:93:53:c2:a4 brd ff:ff:ff:ff:ff:ff
	...
	# 10.103.164.93/32正是config-manager的svc的VIP
	inet 10.103.164.93/32 brd 10.103.164.93 scope global kube-ipvs0
	   valid_lft forever preferred_lft forever
	...
...	
```

接着，kube-proxy通过Linux的ipvs模块，为这个ip地址设置若干个ipvs虚拟主机，并设置虚拟主机之间使用轮询模式（rr)来作为负载均衡策略。而这几个虚拟主机的ip和端口，正是被该service代理的pod地址。此时，任何发放10.103.164.93:80的请求，都会被转发到某一个pod上。

```bash
$ ipvsadm -ln |grep -A 2 10.103.164.93
# config-manager的VIP的暴露端口，正是80
TCP		10.103.164.93:80 RR
  # pod暴露的端口，正是8085
  ->	172.30.2.47:8085				Masq	1	0	0
  ->	172.30.2.183:8085				Masq	1	0	0
```

