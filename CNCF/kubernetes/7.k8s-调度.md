[TOC]

# 调度

Scheduler作为单独的程序运行，启动后监听API server，获取Podspec.NodeName为空的pod，然后进行调度。

## 调度过程

1. 初选，predicate

   从集群所有的节点上，根据调度算法挑选出所有可以运行该Pod的节点。

   predicate阶段没有合适的节点，pod会一直pending，不断重试调度，直到有节点满足条件。如果有多个节点满足条件，就进行优选priority，然后再优先级排序。

   predicate算法：

   - GeneralPredicates。
     - PodFitsResources：宿主机的CPU和内存资源是否够用，是否大于pod的requests字段。
     - PodFitsHost：如果pod指定了NodeName，检查节点名称是否和Pod的spec.nodeName匹配。
     - PodFitsHostPorts：节点上已经使用的port，是否和Pod申请的宿主机端口spec.nodePort冲突。
     - PodMatchNodeSelector：Pod的nodeSelector或者nodeAffinity指定的节点，是否与待考察节点匹配。
   - 与volume相关的过滤规则。
     - NoDiskConflict：多个Pod声明挂载的持久化Volume是否有冲突。比如，AWS EBS类型的Volume，是不允许被两个Pod同时使用的。所以当一个名叫A的EBS Volume已经被挂载在了某个节点上时，另一个同样声明使用这个A Volume的Pod，就不能被调度到这个节点上了。
     - MaxPDVolumeCountPredicate：一个节点上某种类型的持久化Volume是不是已经超过了一定数目。
     - VolumeZonePredicate：检查持久化Volume的Zone（高可用域）标签，是否与待考察节点的Zone标签相匹配。
     - VolumeBindingPredicate：该Pod对应的PV的nodeAffinity字段，是否跟某个节点的标签相匹配。Local Persistent Volume（本地持久化卷），必须使用nodeAffinity来跟某个具体的节点绑定。所以k8s就必须能够根据Pod的Volume属性来进行调度。此外，如果该Pod的PVC还没有跟具体的PV绑定的话，调度器还要负责检查所有待绑定PV，当有可用的PV存在并且该PV的nodeAffinity与待考察节点一致时，这条规则才会返回“成功”。
   - 与宿主机相关的过滤规则。
     - PodToleratesNodeTaints：Pod的Toleration字段与Node的Taint字段是否能够匹配。
     - NodeMemoryPressurePredicate：当前节点的内存是不是已经不够充足。
   - 与Pod相关的过滤规则。
     - PodAffinityPredicate：检查待调度Pod与Node上的已有Pod之间的亲密（affinity）和反亲密（anti-affinity）关系。

2. 优选，priority

   从第一步的结果中，再根据调度算法挑选出所有可以运行该Pod的节点。

   priority算法：

   - LeastRequestedPriority：空闲资源（CPU和Memory）最多的宿主机，得分越高。
   - BalancedResourceAllocation：它选择的是，调度完成后，所有节点里各种资源分配最均衡的那个节点。
   - ImageLocalityPriority：倾向于已经有要使用镜像的节点，镜像总大小越大，权重越高。



## 亲和性

**可以理解为是从pod的角度看调度过程。**有三处“开关”：**node/pod**，**亲和性/互斥性**，**硬策略/软策略**。

- node

  - key：
    - pod.spec.affinity.nodeAffinity：亲和性
    - pod.spec.affinity.nodeAntiAffinity：互斥性
  - value：
    - requireDuringSchedulingIgnoredDuringExecution：硬策略
    - preferredDuringSchedulingIgnoredDuringExecution：软策略

- pod

  支持拓扑域，topologyKey：比如，kubernetes.io/hostname(node的标签)

  - key：
    - pod.spec.affinity.podAffinity：亲和性
    - pod.spec.affinity.podAntiAffinity：互斥性
  - value：
    - requireDuringSchedulingIgnoredDuringExecution：硬策略
    - preferredDuringSchedulingIgnoredDuringExecution：软策略

- 软策略中还支持权重

  ```yaml
  spec:
    affinity:
      nodeAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 1
            preference:
              matchExpressions:
                - key : source
                  operator: In
                  values:
                    - xxxx
  ```

  operator操作符：

  - In：key的值在列表中
  - NotIn：
  - Gt：key的值大于某个值
  - Lt：
  - Exists:key存在
  - DoesNotExist:



## 污点和容忍

**可以理解为从节点的角度看调度。**

- taint，污点

   taint使节点能够排斥一类特定的pod。taint和toleration相互配合，可以用来避免pod被分配到不合适的节点上。

  每个节点都可以应用一个或多个taint，这表示对于那些不能容忍这些taint的pod，是不会被该节点接受的。

  如果将toleration应用于pod上，则表示这些pod可以（但不要求）被调度到具有匹配taint的节点上。

  - node的污点的组成

    key=value:effect（值可以省略）

  - effect的选项

    - NoSchedule：不会将pod调度到具有该污点的node上
    - PreferNoSchedule：尽量避免将pod调度到具有该污点的node上
    - NoExecute：不会调度，且会将已存在的pod驱逐

- toleration，容忍

  pod.spec.tolerations

  - key, value, effect要与node上设置的taint保持一致
  - 当不指定key时，容忍所有的污点key
  - 当不指定effect时，容忍所有的污点作用
  - operator的值为Exists将会忽略value值
  - tolerationSeconds用于描述当pod需要被驱逐时可以在pod上继续运行的时间



## 指明调度节点

指明调度节点，相当于弃用亲和性策略，污点策略。

- pod.spec.nodeName（名字强匹配）
- pod.spec.nodeSelector（标签强匹配）