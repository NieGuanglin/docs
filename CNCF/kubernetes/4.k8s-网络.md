[TOC]

# 网络

## docker项目

### 容器的单机通信

网络栈

一个Linux容器能看见的“网络栈”，实际上是被隔离在它自己的Network Namespace当中的。

所谓“网络栈”，就包括了：网卡（Network Interface），回环设备（Loopback Device），路由表（Routing Table）和iptables规则。对于一个进程来说，这些要素，其实就构成了它发起和响应网络请求的基本环境。

#### docker0网桥

为了避免引入共享网络资源的问题，比如端口冲突，一般容器进程要使用自己的Network Namespace里的网络栈，即拥有自己的IP和端口。这个被隔离的容器进程，该如何跟其他Network Namespace里的容器进程进行交互呢?

在Linux中，能够起到虚拟交换机作用的网络设备，是网桥（Bridge）。**网桥是一个工作在数据链路层（Data Link）的设备，主要功能是根据MAC地址学习来将数据包转发到网桥的不同端口（Port）上。**

在Docker项目中，会在宿主机上创建一个名叫docker0的网桥，凡是连接到docker0网桥上的容器，就可以通过它来进行通信。

#### Veth Pair

**虚拟设备Veth Pair，它被创建出来后，总是以两张虚拟网卡（Veth Peer）的形式成对出现。从其中一个“网卡”发出的数据包，可以直接出现在与它对应的另一张“网卡”上，哪怕这两个“网卡”在不同的Network Namespace里。**

被限制在Network Namespace里的容器进程，实际上是通过Veth Pair容器 + 宿主机网桥的方式，实现了跟其他容器的数据交换。

而且，一旦一张虚拟网卡被插在了网桥了，它就会变成该网桥的“从设备”。从设备会被剥夺调用网络协议栈处理数据包的资格，从而降级成为网桥上的一个端口。这个端口的唯一作用，就是接收流入的数据包，然后由网桥决定转发或者丢弃。

### 容器与容器的单机通信

总结来说，被限制在Network Namespace里的容器进程，实际上是通过Veth Pair设备 + 宿主机网桥的方式，实现了与同主机其他容器的数据交换。

1. 容器1要访问容器2时，目的IP地址会匹配到一条路由规则，该规则规定，凡是匹配到这条规则的IP包，应该经过本机的eth0网卡，通过链路层网络直接发往目的主机。eth0网卡，就是一个Veth Pair，一端在容器1的Network Namespace里，另一端在宿主机上并被插到了宿主机的docker0网桥上。

   ```bash
   # 验证容器内路由规则
   # 在容器内
   $ docker exec containerID ip addr
   eth0: ...
   			inet 172.17.0.4/16 ...
   			...
   $ docker exec containerID ip route
   default via 172.17.0.1 dev eth0
   172.17.0.0/16 dev eth0 scope link src 172.17.0.4
   # eth0网卡是这个容器里的默认路由设备
   # 所有对172.17.0.0/16网段的请求，会交给eth0处理
   
   # 验证容器里对应的Veth Pair设备，在宿主机上是一张虚拟网卡。并被插在了docker0网桥上。
   # 在宿主机上
   $ ifconfig
   docker0:	...
   					inet 172.17.0.1 netmask 255.255.0.0 ...
   					...
   veth6f59990:	...
   veth9fb693d:	...
   vethbc849a0:	...
   $ brctl show
   bridge name	bridge id						STP enabled		interfaces
   docker0			8000.4efae120ab75		no						veth6f59990
   																							veth9fb693d
   																							vethbc849a0
   ```

   

2. 但是要通过二层网络到达容器2，需要有IP地址对应的MAC地址。容器1就通过eth0网卡发送一个ARP广播，通过IP地址查找对应的MAC地址。docker0网桥接收到ARP请求后，会扮演二层交换机的角色，把ARP广播到其他被插到docker0网桥上的虚拟网卡。容器2的网络协议栈收到这个请求后，就把MAC地址回复给容器1。

3. 有了MAC地址，容器1就通过eth0网卡把数据包发给docker0网桥。docker0处理转发的过程，就是继续扮演交换机的角色。根据目的MAC地址，在它的CAM表（交换机通过MAC地址学习维护的端口和MAC地址的对应表）里查到目的端口，然后转发给它。

4. 目的端口，也就是容器2的Veth Pair插在宿主机这头的虚拟网卡，这样数据包就进入到了容器2的Network Namespace里。容器2看到的情况是，自己的eth0网卡上出现了流入的数据包。容器2的网络协议栈就会对请求进行处理。

### 主机与容器的单机通信

#### 本台主机与容器通信

在上文基础上，多了一步docker0网桥到宿主机eth0。

```bash
# 在宿主机上
# 本机路由到docker0，即本机发往容器
$ ip route
...
172.17.0.0/16 dev docker0 proto kernel scope link src 172.17.0.1
...
# docker0路由到本机，即容器发往本机
$ ip route
...
本机网段 dev eth1 ...
...
```

#### 别的主机与容器通信

1. 所有从容器内部发出来的包，都需要做SNAT，将源IP转换为物理网卡的IP。如果有多个容器，所有的容器共享一个外网的IP地址，但是在conntrack表中，记录下这个出去的连接。
2. 服务器返回结果的时候，到达物理机，会根据conntrack表中的规则，取出原来的私网IP，通过DNAT将地址转换为私网IP。再通过docker0网桥实现对内访问。



## kubernetes项目

### Flannel

Flannel项目是CoreOS公司主推的容器网络方案。它支持三种方案为我们提供容器网络功能，来解决跨主通信。

- UDP，三层网络方案。
- VXLAN，二层网络方案。
- host-gw，三层网络方案。

底层的物理网络设备组成的网络，称为**Underlay**网络。基于物理网络的虚拟化网络实现，称为**Overlay**网络。

三层网络方案中，内部数据包就仿佛一直在虚拟的三层网络上流动，即：首先对发出端的IP包进行UDP封装，然后在接收端进行解封装拿到原始的IP包。【虚拟网络封装的内容是三层IP包】

二层网络方案中，内部数据帧就仿佛一直在虚拟的二层网络上流动，即：首先对发出的完整的MAC数据帧封装，然后在接收端进行解封装拿到原始的MAC包，仿佛可以在一个局域网里自由通信。【虚拟网络封装的内容是二层MAC包】

这就是Overlay网络，也即覆盖网络的含义。

#### UDP

##### flannel0

在Linux中，TUN（Tunnel）设备是一种工作在三层的虚拟网络设备。它的功能是，在操作系统内核和用户应用程序之间传递IP包。flannel0就是一个TUN设备。

内核态=》用户态：当操作系统将一个IP包发送给flannel0设备之后，flannel0就会把这个IP包，交给创建这个设备的应用程序，也就是flanneld进程。

用户态=》内核态：如果flanneld进程向flannel0设备发送了一个IP包，那么这个IP包就会出现在宿主机网络栈中，然后根据宿主机的路由表进行下一步处理。

##### 容器与容器的跨主通信

<img src="https://github.com/NieGuanglin/docs/blob/main/pics/CNCF/k8s/容器跨主通信-UDP.png">

<img src="/Users/nieguanglin/docs/pics/CNCF/k8s/容器跨主通信-UDP.png" alt="容器跨主通信-UDP" style="zoom:100%;" />

1. 目的地址不在node1的cni0网桥的网段里，IP包就按默认路由规则，进入cni0网桥（如果是同一台主机上的容器通信，走的是直连规则），出现在宿主机上。

   IP包从容器到docker0网桥，就是用户态==》内核态。

2. Flannel已经在宿主机上创建了一系列的路由规则，IP包根据这些路由规则，进入一个叫flannel0的设备中。

3. 当操作系统将一个IP包发送给flannel0设备之后，flannel0就会把这个IP包，交给flanneld进程。

   这就是内核态==》用户态。

4. 由Flannel管理的容器网络里，一台宿主机上所有的容器，都属于该宿主机被分配的一个子网，子网与宿主机的对应关系被保存在etcd中。flanneld进程在处理由flannel0传入的IP包时，就可以根据目标IP，匹配到对应子网，再根据子网找到宿主机IP。

5. 有了目标宿主机的IP，flanneld就把IP包封装在一个UDP包里，发送给目标主机。端口都是flanneld进程监听的8285端口。

   发送UDP包又要进行系统调用，用户态==》内核态。
   
6. node2收到数据包后，内核栈层层拆包，最终将内部的那个原始ip包发给node2的flanneld进程。

   内核态==》用户态。

7. flanneld进程把原始的ip包发给flannel0设备。

   用户态==》内核态。

8. 内核网络栈根据路由规则，会把这个原始ip包发给cni0网桥。cni0充当二层交换机再将数据包转发给正确的端口，进而通过Veth Pair设备进入容器2。

   内核态==》用户态。

##### UDP方案的缺点

<img src="https://github.com/NieGuanglin/docs/blob/main/pics/CNCF/k8s/容器跨主通信-UDP多次拷贝数据.png">

<img src="/Users/nieguanglin/docs/pics/CNCF/k8s/容器跨主通信-UDP多次拷贝数据.png" alt="容器跨主通信-UDP多次拷贝数据" style="zoom:100%;" />

仅在发出IP包的过程中，就需要经过三次用户态与内核态之间的数据拷贝，性能不好。

#### VXLAN

VXLAN，即Virtual Extensible LAN，虚拟可扩展局域网，是Linux内核本身就支持的一种网络虚拟化技术。VXLAN可以完全在内核态实现上述封装和解封装的工作，从而通过与前面相似的“隧道”机制，构建覆盖网络。

它的设计思想是：在现有的三层网络之上，覆盖一层虚拟的、由内核VXLAN模块负责维护的二层网络，使得连接在这个VXLAN二层网络上的主机（虚拟机或者容器都可以）之间，可以像在同一个局域网（LAN）里那样自由通信。

##### VTEP

即VXLAN Tunnel End Point（虚拟隧道端点），是一个特殊的网络设备作为隧道两端。它进行封装和解封装的对象，是二层数据帧（Ethernet frame）。因为VXLAN本身就是Linux内核中的一个模块，所以这个工作的执行流程，全部是在内核里完成的。

每台宿主机上名叫flannel.1的设备，就是VTEP设备。它既有IP地址，也有MAC地址。

##### 容器与容器的跨主通信

<img src="https://github.com/NieGuanglin/docs/blob/main/pics/CNCF/k8s/容器跨主通信-VXLAN.png">

<img src="/Users/nieguanglin/docs/pics/CNCF/k8s/容器跨主通信-VXLAN.png" alt="容器跨主通信-VXLAN" style="zoom:100%;" />

1. 当container1发出请求后，IP包先出现在docker0网桥。

   ```bash
   # 在node1，查看容器ip
   $ docker exec container1ID ifconfig
   eth0	...
   			inet:172.30.0.75 netmask:255.255.255.255	...
   			...
   # 在node1，查看容器内路由规则，eth0网卡是这个容器的默认路由设备；所有对172.30.0.0/16网段的请求，会交给eth0处理。
   # 这个eth0网卡就是veth pair插在容器的一端，另一端插在cni0网桥。
   $ docker exec container1ID route
   Destination	Gateway			Genmask			Flags	Metric	Ref	Use	Iface
   default			172.30.0.1	0.0.0.0			UG		0				0		0		eth0
   172.30.0.0	0.0.0.0			255.255.0.0	U			0				0		0		eth0
   
   # 在node2，查看容器ip
   $ docker exec container2ID ifconfig
   eth0	...
   			inet:172.30.1.38 netmask:255.255.255.255	...
   			...
   ```

   

2. 每台宿主机上的flanneld进程，会在新节点加入Flannel网络后，在所有其他节点添加路由规则：发往某网段的IP包，都需要经过flannel.1设备发出，下一跳网关是多少。因此，IP包会被路由到本机flannel.1设备，也就是来到了隧道的入口。而这个网关，就是隧道出口VTEP的IP地址。

   ```bash
   # 在node1，查看宿主机路由规则
   $ route |grep flannel.1
   172.30.1.0	172.30.1.0	255.255.255.0	UG	0	0	0	flannel.1
   # 在node2，查看flannel.1设备ip。验证node1上的网关ip就是node2的flannel.1的ip
   $ ifconfig |grep -A 10 flannel.1
   flannel.1: ...
   					inet 172.30.1.0 netmask 255.255.255.255	...
   					...
   ```

   除此之外，flanneld进程还会添加ARP记录：某个flannel.1的IP地址，对应的MAC地址是多少。

   ```bash
   # 在node1，查看node2的flannel.1的MAC
   $ ip neigh show dev flannel.1
   172.30.1.0	lladdr 0a:26:8e:85:05:df	PERMANENT
   ```

   这样，原始IP包就可以通过IP匹配路由获取出口VTEP的IP，再根据ARP记录获取VTEP的MAC。先完成三层封包，与原始ip包的头相同；再完成二层封包，添加MAC头。

   

3. 为了把内部数据帧近一步封装成为宿主机网络里的一个普通数据帧，会在内部数据帧前面，加一个特殊的VXLAN头。VXLAN头里有一个重要的标志VNI，他是VTEP设备识别某个数据帧是否应该归自己处理的重要标志。在这种模式下，VNI默认为1。这就是宿主机上的VTEP设备都叫flannel.1的原因，这里的1，就是VNI的值。

   

4. flannel.1设备接着会向目的主机发送UDP包。源端口，目的端口都是每台宿主机上flanneld进程监听的端口。完成四层封包。

   

5. 这时，flannel.1设备会扮演网桥的角色，在二层网络进行UDP包的转发。网桥转发的依据，来自转发数据库，FDB，Forwarding Database。flanneld进程负责维护flannel.1网桥的FDB。表中记录会指明规则：某个flannel.1的MAC，对应的主机IP是多少。完成三层封包。

   ```bash
   # 在node1，使用node2的flannel.1的MAC，查看node2的ip
   $ bridge fdb show flannel.1 |grep 0a:26:8e:85:05:df
   0a:26:8e:85:05:df	dev flannel.1 dst	node2ip	self	permanent
   ```

   

6. 通过ARP获取目的主机的MAC。完成二层封包。这样就可以把这个数据帧通过宿主机网络发给目的主机了。

   

7. 这个数据帧先来到目的主机的eth0网卡，内核网络栈层层拆包，发现VXLAN Header，并且VNI为1。内核对它拆包，拿到内部数据帧，交给flannel.1设备。至此，数据包到达隧道的出口。

   

8. flannel.1设备近一步拆包，取出原始IP包。根据路由匹配，转发给docker0网桥。最终转发给目的容器container2。

#### host-gw

- 原理：

  主机（host）会充当容器通信路径里的网关（gateway），这就是host-gw的含义。它的工作原理，就是在主机路由表里，将每个Flannel子网的下一跳，设置为该子网对应的宿主机的IP地址。

  Flannel子网和主机的信息，保存在etcd中。flanneld进程watch这些数据的变化，然后更新维护路由表。

- 容器与容器的跨主通信细节：

  1. IP包从容器1内到达网桥。
  2. 在宿主机配置了路由规则，目的IP地址属于node2中容器子网网段的IP包，应该经过node1的eth0发出，下一跳就是node2的IP。所以，原IP包的目的IP不变，还是容器2的IP。目的MAC就是node2的MAC地址。完成二层封包，从eth0发往node2。
  3. node2收到后，拆掉MAC头。根据路由规则，会把IP包交给网桥。
  4. 网桥转发给容器2。

### Calico

- 原理：

  不同于Flannel通过etcd和宿主机上的flanneld来维护路由信息，Calico项目使用BGP协议来自动地在整个集群中分发路由信息。



三种网络：

- node网络
- pod网络
- service网络